{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "# from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Input,LSTM, Dense, Flatten, Conv1D, Lambda, Reshape\n",
    "from keras.layers.merge import concatenate, multiply,add\n",
    "import tensorflow as tf\n",
    "from keras import regularizers\n",
    "from keras.initializers import glorot_uniform\n",
    "from tqdm import tqdm\n",
    "import rpy2\n",
    "import rpy2.robjects.numpy2ri\n",
    "from stldecompose import decompose\n",
    "rpy2.robjects.numpy2ri.activate()\n",
    "from rpy2.robjects.packages import importr\n",
    "from keras import regularizers\n",
    "\n",
    "from rpy2.robjects import r, pandas2ri\n",
    "pandas2ri.activate()\n",
    "stats = importr('stats')\n",
    "stl=stats.stl\n",
    "ts =stats.ts\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "d =\"\" # path where data file resides\n",
    "data = pd.read_csv(d+\"NN5_interpolated.csv\",header=None)\n",
    "predictions = pd.read_csv(d+'final_theta/theta_50_h3.csv',index_col=0,skiprows = [1])\n",
    "\n",
    "data_length = data.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_input(data,window_size,horizon=1):\n",
    "    length=data.shape[0]\n",
    "    y = np.zeros([length-window_size+1-horizon,horizon])\n",
    "    output=np.zeros([length-window_size+1-horizon,window_size])\n",
    "    for i in range(length-window_size-horizon+1):\n",
    "        output[i:i+1,:]=data[i:i+window_size]\n",
    "        y[i,:]= data[i+window_size:i+window_size+horizon]\n",
    "    return output.reshape(output.shape[0],window_size,1), y\n",
    "\n",
    "def make_k_input(data,window_size,horizon):\n",
    "    length = data.shape[0]\n",
    "    output= np.zeros([length-window_size+1-horizon,horizon])\n",
    "    for i in range(length-window_size-horizon+1):\n",
    "        output[i:i+1,:]=data[i+window_size:i+window_size+horizon]\n",
    "    return output.reshape(output.shape[0],horizon,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonov_make_input(data,window_size,horizon=1):\n",
    "    length=data.shape[0]-window_size\n",
    "    loop=length//horizon\n",
    "    extra = length%horizon\n",
    "\n",
    "    data = np.append(data,np.zeros([horizon-extra]))\n",
    "\n",
    "    if extra ==0:\n",
    "        i_val = loop\n",
    "    else:\n",
    "        i_val=loop+1\n",
    "        \n",
    "    output=np.zeros([i_val,window_size])\n",
    "    y=np.zeros([i_val,horizon])\n",
    "    for i in range(i_val):\n",
    "        output[i:i+1,:]=data[i*horizon:(i*horizon)+window_size]\n",
    "        y[i,:]= data[(i*horizon)+window_size:(i*horizon)+window_size+horizon]\n",
    "        \n",
    "    return output.reshape(output.shape[0],window_size,1), y\n",
    "\n",
    "def nonov_make_k_input(data,window_size,horizon):\n",
    "    length = data.shape[0]-window_size\n",
    "    loop=length//horizon\n",
    "    extra = length%horizon\n",
    "    data_app = np.repeat(data[-1],extra)\n",
    "    data = np.append(data,data_app)    \n",
    "\n",
    "    if extra ==0:\n",
    "        i_val = loop\n",
    "    else:\n",
    "        i_val=loop+1\n",
    "    output=np.zeros([i_val,horizon])\n",
    "    for i in range(i_val):\n",
    "        output[i:i+1,:]=data[(i*horizon)+window_size:(i*horizon)+window_size+horizon]\n",
    "    return output.reshape(output.shape[0],horizon,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 111/111 [11:13<00:00,  6.07s/it]\n"
     ]
    }
   ],
   "source": [
    "with tqdm(total=data_length) as pbar:\n",
    "    \n",
    "        \n",
    "    max_test_samples = 56    \n",
    "    \n",
    "\n",
    "    final_predictions = np.zeros([data_length,max_test_samples])\n",
    "    for y in range(data_length):\n",
    "\n",
    "#             n_test = data.iloc[y].values[2] #----cif\n",
    "        \n",
    "        n_test = 56 # number of test samples in the data\n",
    "        horizon=56 # can be varied, horizon to be predicted by one input window\n",
    "\n",
    "        window_size=25 # number of past values to be used for prediction\n",
    "#             if horizon==6:\n",
    "#                 window_size=7\n",
    "#             else:\n",
    "#                 window_size=15\n",
    "\n",
    "\n",
    "        nn_val= np.asarray(data.iloc[y].dropna().values,dtype=float) #data in one time series\n",
    "        rr = nn_val.size\n",
    "        rr = int(np.floor(rr*.25))\n",
    "        temp1=nn_val[rr:] # if taking 75% of the data, if 50% is required then temp1=nn_val[2*rr:]\n",
    "#             temp1=np.asarray(data.loc[y].dropna().values,dtype=float)[300:]\n",
    "\n",
    "        epsilon = 0.05\n",
    "        temp1[temp1<epsilon] = temp1[temp1<epsilon]+0.05\n",
    "        series = np.log(temp1)\n",
    "#             series = temp1\n",
    "\n",
    "\n",
    "\n",
    "        frequency = 7   # should be adjusted according to dataset       \n",
    "        if temp1.size < 2*frequency:\n",
    "            frequency=2\n",
    "\n",
    "        result = stl(ts(series,frequency=frequency),\"periodic\")\n",
    "        temp=pandas2ri.ri2py(result.rx2('time.series'))\n",
    "        series_1  = temp[:,0]+temp[:,2] # extracting the seasonality and the residual component from the data\n",
    "        \n",
    "        # defining train,test and validation split\n",
    "        t_v_data = series_1[:-n_test]            \n",
    "        series_length = t_v_data.size\n",
    "        n_val = int(np.round(series_length*.2))\n",
    "        if n_val < horizon:\n",
    "            n_val = horizon\n",
    "        train = t_v_data[:-n_val]\n",
    "        if train.size <11:\n",
    "            window_size=3\n",
    "\n",
    "        test = series_1[-(n_test+window_size):]            \n",
    "\n",
    "        val = t_v_data[-(n_val+window_size):]\n",
    "#             resea=temp[:,1][-n_test:]\n",
    "\n",
    "        # preparing knowledge based predictions\n",
    "\n",
    "        temp_theta1= np.asarray(predictions.iloc[y].dropna().values,dtype=float)\n",
    "        temp_theta1[np.argwhere(temp_theta1<=0)]=0.5\n",
    "        temp_theta1 = np.log(temp_theta1)\n",
    "        result_k = stl(ts(temp_theta1,frequency=frequency),\"periodic\")\n",
    "        temp_theta=pandas2ri.ri2py(result_k.rx2('time.series'))\n",
    "        series_k_org = temp_theta[:,0]+temp_theta[:,2]\n",
    "\n",
    "\n",
    "#             temp_theta= np.log(temp_theta)-temp[:,0]\n",
    "        temp2= series_k_org[:-n_test]\n",
    "        test_theta = series_k_org[-(n_test+window_size):]\n",
    "        resea=temp[:,1][-n_test:]\n",
    "        temp2_train = temp2[:-n_val]\n",
    "        temp2_val = temp2[-(n_val+window_size):]\n",
    "        \n",
    "        # making rolling window based input vectors\\\n",
    "        \n",
    "        train_sequence = make_input(train, window_size,horizon)\n",
    "        val_sequence = make_input(val,window_size,horizon)\n",
    "        test_sequence = nonov_make_input(test,window_size,horizon)\n",
    "\n",
    "        k_train = make_k_input(temp2_train,window_size,horizon)               \n",
    "        k_val = make_k_input(temp2_val,window_size,horizon)       \n",
    "        k_test = nonov_make_k_input(test_theta,window_size,horizon)\n",
    "       \n",
    "\n",
    "        x_train = train_sequence[0]\n",
    "        y_train =train_sequence[1]\n",
    "        x_val = val_sequence[0]\n",
    "        y_val = val_sequence[1]\n",
    "        x_test = test_sequence[0]\n",
    "        y_test = test_sequence[1]\n",
    "        \n",
    "        # conditioning input on expert knowledge\n",
    "        \n",
    "        train_input = np.append(x_train,k_train,axis=1)\n",
    "        val_input = np.append(x_val,k_val,axis=1)\n",
    "        test_input = np.append(x_test,k_test,axis=1)\n",
    "\n",
    "\n",
    "        k_train=k_train.reshape(k_train.shape[0],horizon)\n",
    "        k_val=k_val.reshape(k_val.shape[0],horizon)\n",
    "        k_test=k_test.reshape(k_test.shape[0],horizon)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "        K.clear_session()\n",
    "\n",
    "        inputs_n= Input(batch_shape=(None,window_size+horizon,1),name='input_n')\n",
    "        inputs_k=Input(batch_shape=(None,horizon),name='input_k')\n",
    "        branch_0 = Conv1D(64,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(inputs_n)\n",
    "        branch_0 = Conv1D(64,3, strides=1, padding='same',activation='relu',kernel_initializer=glorot_uniform(1))(branch_0)\n",
    "        branch_0=Flatten()(branch_0)\n",
    "        net= Dense(horizon,name='dense_final',activity_regularizer=regularizers.l2(0.03))(branch_0) #activity regularizer value based on data\n",
    "        net=add([net,inputs_k])\n",
    "    \n",
    "        model=Model(inputs=[inputs_n,inputs_k],outputs=net)\n",
    "        opt = Adam(lr=0.0001)\n",
    "        callback = ModelCheckpoint(filepath=d+str(y)+'_nn5.h5',monitor='val_loss',save_best_only=True,save_weights_only=True)\n",
    "\n",
    "        model.compile(loss='mean_squared_error',optimizer=opt)\n",
    "\n",
    "        model.fit({'input_n':train_input, 'input_k':k_train},y_train,validation_data=[[val_input,k_val],y_val],callbacks=[callback],batch_size=8,shuffle=True, epochs=75,verbose=0)\n",
    "\n",
    "        model.load_weights(d+str(y)+'_nn5.h5')\n",
    "        pred=model.predict({'input_n':test_input, 'input_k':k_test})\n",
    "\n",
    "        pred=pred.reshape(pred.size)[:n_test]\n",
    "\n",
    "        final_predictions[y,:n_test] = pred.reshape(n_test)\n",
    "        pbar.update(1)\n",
    "    np.savetxt(d+'final/50_horg_nonov.csv',final_predictions, fmt='%1.3f',delimiter=',')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
